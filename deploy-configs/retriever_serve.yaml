# Ray Serve config file
#
# For documentation see: 
# https://docs.ray.io/en/latest/serve/production-guide/config.html

# Host and port for the HTTP proxy
http_options: 
  host: 127.0.0.1
  port: 8000

applications:
- name: RayRetriever
  route_prefix: /
  import_path: ray_retriever.serve.retriever:deployment

  runtime_env:
    env_vars:
      # Without this variable huggingface/tokenizers logs a warning: 
      # "The current process just got forked, after parallelism has already been used."
      # See: https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning
      TOKENIZERS_PARALLELISM: "false"
      OPENAI_API_KEY: ${OPENAI_KEY}
      ANYSCALE_ENDPOINT_KEY: ${ANYSCALE_KEY}
      LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PK}
      LANGFUSE_SECRET_KEY: ${LANGFUSE_SK}

  args:
    embedding_batch_size: 32
    embedding_batch_wait_timeout_s: 0.1
    weaviate_hostname: 127.0.0.1
    weaviate_port: 9001
    index_name: Wikipedia
    similarity_top_n: 20
    rerank_top_n: 3
    rerank_batch_size: 32
    response_prompt_name: generate_response[Llama-2-70b]

  deployments:
  - name: EmbeddingGenerator
    num_replicas: 1
    max_concurrent_queries: 10
    ray_actor_options:
      num_cpus: 1
      num_gpus: 0
  - name: SearchEngine
    num_replicas: 1
    max_concurrent_queries: 10
    ray_actor_options:
      num_cpus: 1
  - name: Reranker
    num_replicas: 1
    max_concurrent_queries: 10
    ray_actor_options:
      num_cpus: 1
      num_gpus: 0
  - name: ResponseGenerator
    num_replicas: 1
    max_concurrent_queries: 10
    ray_actor_options:
      num_cpus: 1
  - name: Retriever
    num_replicas: 1
    max_concurrent_queries: 10
    ray_actor_options:
      num_cpus: 1