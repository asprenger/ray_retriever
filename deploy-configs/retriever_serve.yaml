# Serve config file
#
# For documentation see: 
# https://docs.ray.io/en/latest/serve/production-guide/config.html

# Host and port for the HTTP proxy
http_options: 
  host: 127.0.0.1
  port: 8000

applications:
- name: RayRetriever
  route_prefix: /
  import_path: ray_retriever.serve.retriever:deployment

  runtime_env:
    env_vars:
      # Without this variable huggingface/tokenizers logs a warning: 
      # "The current process just got forked, after parallelism has already been used."
      # See: https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning
      TOKENIZERS_PARALLELISM: "false"

  args:
    embedding_batch_size: 32
    embedding_batch_wait_timeout_s: 0.1
    weaviate_hostname: 127.0.0.1
    weaviate_port: 9001
    index_name: Wikipedia
    similarity_top_n: 20
    rerank_top_n: 3
    rerank_batch_size: 32
    # Anyscale models:
    # meta-llama/Llama-2-7b-chat-hf
    # meta-llama/Llama-2-13b-chat-hf
    # meta-llama/Llama-2-70b-chat-hf
    # codellama/CodeLlama-34b-Instruct-hf
    # mistralai/Mistral-7B-Instruct-v0.1
    # mistralai/Mixtral-8x7B-Instruct-v0.1
    response_model: meta-llama/Llama-2-70b-chat-hf
    anyscale_endpoint_key: {ANYSCALE_ENDPOINT_KEY}
    #response_model: gpt-3.5-turbo
    #openai_api_key: {OPENAI_API_KEY}

  deployments:
  - name: EmbeddingGenerator
    num_replicas: 1
    max_concurrent_queries: 10
    ray_actor_options:
      num_cpus: 1
      num_gpus: 0
  - name: SearchEngine
    num_replicas: 1
    max_concurrent_queries: 10
    ray_actor_options:
      num_cpus: 1
  - name: Reranker
    num_replicas: 1
    max_concurrent_queries: 10
    ray_actor_options:
      num_cpus: 1
      num_gpus: 0
  - name: ResponseGenerator
    num_replicas: 1
    max_concurrent_queries: 10
    ray_actor_options:
      num_cpus: 1
  - name: Retriever
    num_replicas: 1
    max_concurrent_queries: 10
    ray_actor_options:
      num_cpus: 1